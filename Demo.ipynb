{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative imports\n",
    "from uncertainty_aware_diagnosis import (\n",
    "    ICD10data,\n",
    "    SimpleMLP,\n",
    "    sklearnMLP,\n",
    ")\n",
    "\n",
    "# absolute imports\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchmetrics import F1Score, Recall, Accuracy\n",
    "\n",
    "# paths\n",
    "train_csv = \"./data/train.csv\"\n",
    "val_csv = \"./data/val.csv\"\n",
    "test_csv = \"./data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "batch_size = 32\n",
    "hidden_dim = 256\n",
    "shuffle = True\n",
    "\n",
    "num_epochs = 10\n",
    "early_stopping_patience = 5\n",
    "dropout = 0.2\n",
    "learning_rate = 1e-3\n",
    "scheduler_step_size = 5\n",
    "scheduler_gamma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ds_train = ICD10data(train_csv)\n",
    "ds_val = ICD10data(val_csv)\n",
    "ds_test = ICD10data(test_csv)\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=shuffle)\n",
    "val_loader = DataLoader(ds_val, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "input_dim = ds_train.X.shape[1]\n",
    "output_dim = ds_train.classes.shape[0]\n",
    "\n",
    "print(f\"(input_dim: {input_dim}, hidden_dim: {hidden_dim}, output_dim: {output_dim})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = SimpleMLP(\n",
    "    input_dim=input_dim, hidden_dim=hidden_dim, num_classes=output_dim, dropout=dropout\n",
    ")\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer,\n",
    "#     step_size=scheduler_step_size,\n",
    "#     gamma=scheduler_gamma\n",
    "# )\n",
    "# metrics\n",
    "f1_macro = F1Score(task=\"multiclass\", average=\"macro\", num_classes=output_dim)\n",
    "recall_macro = Recall(task=\"multiclass\", average=\"macro\", num_classes=output_dim)\n",
    "\n",
    "# counters\n",
    "best_f1 = -np.inf\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training loop\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    f1_macro.reset()\n",
    "    recall_macro.reset()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, y) in enumerate(val_loader):\n",
    "            logits = model(X)\n",
    "            val_loss += criterion(logits, y).item() * X.size(0)\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "    # calculate metrics\n",
    "    y_pred = torch.argmax(logits, dim=1)\n",
    "    f1_macro.update(y_pred, y)\n",
    "    recall_macro.update(y_pred, y)\n",
    "\n",
    "    # print progress\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, F1 Macro: {f1_macro.compute():.4f}, Recall Macro: {recall_macro.compute():.4f}\"\n",
    "    )\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    # scheduler.step()  # (If using ReduceLROnPlateau: scheduler.step(val_macro_f1))\n",
    "\n",
    "    # check for improvement for early stopping\n",
    "    if f1_macro.compute() > best_f1:\n",
    "        best_f1 = f1_macro.compute()\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "if \"best_model_state\" in locals():\n",
    "    # load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    # save best model\n",
    "    # torch.save(best_model_state, 'models/best_model.pth')\n",
    "\n",
    "print(\n",
    "    \"=================================================================================\\n\"\n",
    ")\n",
    "print(f\"Best F1 Macro: {best_f1:.4f}, Recall Macro: {recall_macro.compute():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test best model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "f1_macro.reset()\n",
    "recall_macro.reset()\n",
    "accuracy = Accuracy(task=\"multiclass\", average=\"macro\", num_classes=output_dim)\n",
    "\n",
    "# Prepare a list to store predictions\n",
    "predicted_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (X, y) in enumerate(test_loader):\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        test_loss += loss.item() * X.size(0)\n",
    "        y_pred = torch.argmax(logits, dim=1)\n",
    "        predicted_probs.append(logits)\n",
    "        f1_macro.update(y_pred, y)\n",
    "        recall_macro.update(y_pred, y)\n",
    "        accuracy.update(y_pred, y)\n",
    "\n",
    "# compute final values\n",
    "test_acc = accuracy.compute()\n",
    "test_f1 = f1_macro.compute()\n",
    "test_recall = recall_macro.compute()\n",
    "test_probs = torch.cat(predicted_probs)\n",
    "\n",
    "print(f\"Test Accuracy:          {test_acc:.4f}\")\n",
    "print(f\"Test Macro-F1:          {test_f1:.4f}\")\n",
    "print(f\"Test Macro-Recall:      {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration imports\n",
    "from pycalib.visualisations import plot_reliability_diagram\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from pycalib.models.calibrators import LogisticCalibration\n",
    "from pycalib.metrics import ECE, classwise_ECE, conf_ECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ds_train.X.cpu().numpy()\n",
    "y_train = ds_train.y.cpu().numpy()\n",
    "X_val = ds_val.X.cpu().numpy()\n",
    "y_val = ds_val.y.cpu().numpy()\n",
    "X_test = ds_test.X.cpu().numpy()\n",
    "y_test = ds_test.y.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "num_epochs = 25\n",
    "early_stopping_patience = 10\n",
    "learn_rate = 1e-3\n",
    "dropout = 0.2\n",
    "hidden_dim = 256\n",
    "\n",
    "model = SimpleMLP(\n",
    "    input_dim=input_dim, hidden_dim=hidden_dim, num_classes=output_dim, dropout=dropout\n",
    ")\n",
    "model.fit(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    early_stopping_patience=early_stopping_patience,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(ds_test.X)\n",
    "y_proba = model.predict_proba(ds_test.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_reliability_diagram(\n",
    "    y_test,\n",
    "    [\n",
    "        y_proba,\n",
    "    ],\n",
    "    legend=[\n",
    "        \"MLP\",\n",
    "    ],\n",
    "    class_names=list(ds_test.classes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) get your raw logits from the MLP\n",
    "props_val = model.predict_proba(ds_val.X)  # shape (n_val, n_classes)\n",
    "probs_test = model.predict_proba(ds_test.X)  # shape (n_test, n_classes)\n",
    "\n",
    "# 2) instantiate & fit the pycalib logistic (Platt) calibrator\n",
    "calibrator = LogisticCalibration(\n",
    "    C=0.002, solver=\"lbfgs\", multi_class=\"multinomial\", log_transform=True\n",
    ")\n",
    "calibrator.fit(props_val, y_val)\n",
    "\n",
    "# 3) use it to get calibrated probabilities on your test set\n",
    "probs_calibrated = calibrator.predict_proba(probs_test)  # shape (n_test, n_classes)\n",
    "\n",
    "_ = plot_reliability_diagram(\n",
    "    ds_test.y.numpy(),\n",
    "    [probs_test, probs_calibrated],\n",
    "    legend=[\"MLP\", \"+ Calibrator\"],\n",
    "    class_names=list(ds_test.classes),\n",
    ")\n",
    "\n",
    "for metric in ECE, conf_ECE, classwise_ECE:\n",
    "    print(metric.__name__)\n",
    "    print(\"Classifier = {:.3f}\".format(metric(ds_test.y.numpy(), probs_test, bins=15)))\n",
    "    print(\n",
    "        \"Calibrator = {:.3f}\".format(\n",
    "            metric(ds_test.y.numpy(), probs_calibrated, bins=15)\n",
    "        )\n",
    "    )\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[explains ECE and nessesary binning](https://towardsdatascience.com/expected-calibration-error-ece-a-step-by-step-visual-explanation-with-python-code-c3e9aa12937d/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert calibrated probabilities to predicted classes\n",
    "y_pred_calibrated = torch.argmax(torch.tensor(probs_calibrated), dim=1)\n",
    "\n",
    "# Initialize metrics\n",
    "f1_macro = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(ds_test.classes))\n",
    "recall_macro = Recall(\n",
    "    task=\"multiclass\", average=\"macro\", num_classes=len(ds_test.classes)\n",
    ")\n",
    "\n",
    "# Update metrics with predictions and true labels\n",
    "f1_macro.update(y_pred_calibrated, ds_test.y)\n",
    "recall_macro.update(y_pred_calibrated, ds_test.y)\n",
    "\n",
    "# Compute final values\n",
    "final_f1 = f1_macro.compute()\n",
    "final_recall = recall_macro.compute()\n",
    "\n",
    "# Print results\n",
    "print(f\"F1 Macro: {final_f1:.4f}\")\n",
    "print(f\"Recall Macro: {final_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs_test[0])\n",
    "print(probs_calibrated[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get confidence interval from point estimation per class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = CalibratedClassifierCV(estimator=model, method=\"sigmoid\", cv=\"prefit\")\n",
    "cal.fit(ds_val.X.numpy(), ds_val.y.numpy())\n",
    "\n",
    "scores_cal = cal.predict_proba(ds_test.X)\n",
    "\n",
    "_ = plot_reliability_diagram(\n",
    "    ds_test.y.numpy(),\n",
    "    [test_probs.numpy(), scores_cal],\n",
    "    legend=[\"MLP\", \"+ Calibrator\"],\n",
    "    class_names=list(ds_test.classes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) pull out your arrays from the Dataset\n",
    "#    (they’re already torch.Tensors of shape [n_samples, n_features])\n",
    "X_train = ds_train.X.cpu().numpy()\n",
    "y_train = ds_train.y.cpu().numpy()\n",
    "X_val = ds_val.X.cpu().numpy()\n",
    "y_val = ds_val.y.cpu().numpy()\n",
    "X_test = ds_test.X.cpu().numpy()\n",
    "y_test = ds_test.y.cpu().numpy()\n",
    "\n",
    "# 2) instantiate the sklearn‐wrapped MLP\n",
    "mlp = sklearnMLP(\n",
    "    input_dim=X_train.shape[1],  # number of columns in X_train\n",
    "    hidden_dim=128,  # pick whatever you like\n",
    "    num_classes=len(ds_train.classes),  # your LabelEncoder’s classes\n",
    "    batch_size=64,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-3,\n",
    "    early_stopping_patience=5,\n",
    "    device=\"cpu\",  # or \"cuda\" if you like\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 3) fit on your training set\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) calibrate on the validation set\n",
    "cal = CalibratedClassifierCV(estimator=mlp, method=\"sigmoid\", cv=\"prefit\")\n",
    "cal.fit(X_val, y_val)\n",
    "\n",
    "# 5) now you can predict or get calibrated probabilities\n",
    "probs_test_uncal = mlp.predict_proba(X_test)  # raw MLP probs\n",
    "probs_test_cal = cal.predict_proba(X_test)  # after sigmoid‐calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertainty_aware_diagnosis import SklearnWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out your arrays\n",
    "X_train, y_train = ds_train.X.numpy(), ds_train.y.numpy()\n",
    "X_val, y_val = ds_val.X.numpy(), ds_val.y.numpy()\n",
    "X_test, y_test = ds_test.X.numpy(), ds_test.y.numpy()\n",
    "\n",
    "# wrap it\n",
    "sk_mlp = SklearnWrapper(\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dim=128,\n",
    "    num_classes=len(ds_train.classes),\n",
    "    batch_size=64,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-3,\n",
    "    device=\"cpu\",  # or \"cuda\"\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 1) fit\n",
    "sk_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) calibrate\n",
    "cal = CalibratedClassifierCV(sk_mlp, method=\"sigmoid\", cv=\"prefit\")\n",
    "cal.fit(X_val, y_val)\n",
    "\n",
    "# 3) now you can do\n",
    "probs_uncal = sk_mlp.predict_proba(X_test)\n",
    "probs_cal = cal.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_reliability_diagram(\n",
    "    ds_test.y.numpy(),\n",
    "    [probs_test_uncal, probs_test_cal],\n",
    "    legend=[\"MLP\", \"+ Calibrator\"],\n",
    "    class_names=list(ds_test.classes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
