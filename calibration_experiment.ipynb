{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- drop NaN values\n",
    "- removed 3rd gender (n=1)\n",
    "- merge rare classes (~4000 classes --> 1000)\n",
    "- drop classes with less than 3x25 samples\n",
    "- stratified 70% train, 15% calibration, 15% test split\n",
    "\n",
    "\n",
    "data_config: dictionary containing the following keys:\n",
    "```\n",
    "target: target column name\n",
    "numerical_features: list of numerical feature names\n",
    "categorical_features: list of categorical feature names\n",
    "high_cardinality_features: list of high cardinality feature names, that must be embeded (too large feature space for one hote encoding (ohe))\n",
    "use_embedding: bool whether to use embedding for high cardinality features\n",
    "\n",
    "train_csv: str path to training csv file\n",
    "val_csv: str path to validation csv file\n",
    "test_csv: sre path to test csv file\n",
    "ohe_pkl: str path to pickle file containing ohe categories\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data configuration\n",
    "from data.config import data_config\n",
    "\n",
    "# relative imports\n",
    "from uncertainty_aware_diagnosis import(\n",
    "    ICD10data, \n",
    "    SimpleMLP, \n",
    "    PlattCalibrator,\n",
    "    TemperatureScaling,\n",
    ")\n",
    "\n",
    "# absolute imports\n",
    "import polars as pl\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchmetrics import F1Score, Recall\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from pycalib.visualisations import plot_reliability_diagram\n",
    "from pycalib.models.calibrators import LogisticCalibration\n",
    "from pycalib.metrics import classwise_ECE, conf_ECE\n",
    "\n",
    "# paths\n",
    "train_csv = data_config['train_csv']\n",
    "val_csv = data_config['val_csv']\n",
    "test_csv = data_config['test_csv']\n",
    "ohe_pkl = data_config['ohe_pkl']\n",
    "\n",
    "# variables dataloader\n",
    "batch_size = 32\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Experiment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different experiment settings\n",
    "predict_hoofdgroepen_only = False\n",
    "use_single_hospital = False\n",
    "use_subset = False # use subset of data for faster training\n",
    "subset_size = 6400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "if predict_hoofdgroepen_only:\n",
    "    target = \"hoofdgroep\"  # simplify multi-class problem by predicting hoofdgroepen instead of specific codes\n",
    "else:\n",
    "    target = data_config['target'] # ICD10 principle diagnosis code\n",
    "\n",
    "# features\n",
    "numerical = data_config['numerical_features']\n",
    "categorical = data_config['categorical_features']\n",
    "high_cardinality_features = data_config['high_cardinality_features']\n",
    "use_embedding = False\n",
    "\n",
    "with open(ohe_pkl, \"rb\") as f:\n",
    "    ohe_cats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ICD10data(\n",
    "    csv_path=train_csv,\n",
    "    numerical=numerical,\n",
    "    categorical=categorical,\n",
    "    high_card=[],\n",
    "    target=target,\n",
    "    dropna=True,\n",
    "    use_embedding=False,\n",
    "    ohe_categories=ohe_cats,  # use one-hot encoded categorie of full dataset\n",
    ")\n",
    "val = ICD10data(\n",
    "    csv_path=val_csv,\n",
    "    numerical=numerical,\n",
    "    categorical=categorical,\n",
    "    high_card=[],\n",
    "    target=target,\n",
    "    dropna=True,\n",
    "    use_embedding=False,\n",
    "    ohe_categories=ohe_cats,  # use one-hot encoded categorie of full dataset\n",
    "    encoder=train.encoder,  # use encoder from training set\n",
    "    scaler=train.scaler,  # use scalor from train set\n",
    ")\n",
    "test = ICD10data(\n",
    "    csv_path=test_csv,\n",
    "    numerical=numerical,\n",
    "    categorical=categorical,\n",
    "    high_card=[],\n",
    "    target=target,\n",
    "    dropna=True,\n",
    "    use_embedding=False,\n",
    "    ohe_categories=ohe_cats,  # use one-hot encoded categorie of full dataset\n",
    "    encoder=train.encoder,  # use encoder from training set\n",
    "    scaler=train.scaler,  # use scalor from train set\n",
    ")\n",
    "\n",
    "input_dim = train.X.shape[1]\n",
    "output_dim = train.classes.shape[0]\n",
    "\n",
    "print(f\"Number of icd10 classes: {len(train.classes)}\")\n",
    "print(f\"(input_dim: {input_dim}, output_dim: {output_dim})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_subset:\n",
    "    train.X = train.X[:subset_size]\n",
    "    val.X = val.X[:subset_size]\n",
    "    train.y = train.y[:subset_size]\n",
    "    val.y = val.y[:subset_size]\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=shuffle)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### select single hospital (if True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_single_hospital:\n",
    "\n",
    "    train = ICD10data(\n",
    "        csv_path=train_csv[:-4] + \"_1hosp\" + \".csv\",\n",
    "        numerical=numerical,\n",
    "        categorical=categorical,\n",
    "        high_card=[],\n",
    "        target=target,\n",
    "        dropna=True,\n",
    "        use_embedding=False,\n",
    "        ohe_categories=ohe_cats,  # use one-hot encoded categorie of full dataset\n",
    "    )\n",
    "    val = ICD10data(\n",
    "        csv_path=val_csv[:-4] + \"_1hosp\" + \".csv\",\n",
    "        numerical=numerical,\n",
    "        categorical=categorical,\n",
    "        high_card=[],\n",
    "        target=target,\n",
    "        dropna=True,\n",
    "        use_embedding=False,\n",
    "        ohe_categories=ohe_cats,  # use one-hot encoded categorie of full dataset\n",
    "        encoder=train.encoder,  # use encoder from training set\n",
    "        scaler=train.scaler,  # use scalor from train set\n",
    "    )\n",
    "    test = ICD10data(\n",
    "        csv_path=test_csv[:-4] + \"_1hosp\" + \".csv\",\n",
    "        numerical=numerical,\n",
    "        categorical=categorical,\n",
    "        high_card=[],\n",
    "        target=target,\n",
    "        dropna=True,\n",
    "        use_embedding=False,\n",
    "        ohe_categories=ohe_cats,  # use one-hot encoded categorie of full dataset\n",
    "        encoder=train.encoder,  # use encoder from training set\n",
    "        scaler=train.scaler,  # use scalor from train set\n",
    "    )\n",
    "\n",
    "    input_dim = train.X.shape[1]\n",
    "    output_dim = train.classes.shape[0]\n",
    "\n",
    "    print(f\"Number of icd10 classes: {len(train.classes)}\")\n",
    "    print(f\"(input_dim: {input_dim}, output_dim: {output_dim})\")\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables MLP\n",
    "num_epochs = 150\n",
    "early_stopping_patience = 20\n",
    "learning_rate = 1e-3\n",
    "dropout = 0.2\n",
    "hidden_dim = 256\n",
    "k_folds = 3\n",
    "\n",
    "# define model\n",
    "model = SimpleMLP(\n",
    "    input_dim=input_dim, hidden_dim=hidden_dim, num_classes=output_dim, dropout=dropout\n",
    ")\n",
    "# # fit model using validation/calibration set\n",
    "# model.fit(\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     num_epochs=num_epochs,\n",
    "#     learning_rate=learning_rate,\n",
    "#     early_stopping_patience=early_stopping_patience,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# fit model using cross validation\n",
    "model.fit_cv(\n",
    "    train_loader,\n",
    "    k_folds=k_folds,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    early_stopping_patience=early_stopping_patience,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# predict y, probs, and define y_test\n",
    "y_pred = model.predict(test.X)\n",
    "y_proba = model.predict_proba(test.X)\n",
    "y_test = test.y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Fit Platt's scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables calibrator\n",
    "C = 0.03\n",
    "solver = \"lbfgs\"\n",
    "\n",
    "# 1) get your raw logits from the MLP\n",
    "props_val = model.predict_proba(val.X)  # shape (n_val, n_classes)\n",
    "probs_test = model.predict_proba(test.X)  # shape (n_test, n_classes)\n",
    "\n",
    "# 2) instantiate & fit the pycalib logistic (Platt) calibrator\n",
    "calibrator = LogisticCalibration(\n",
    "    C=C, solver=solver, multi_class=\"multinomial\", log_transform=True\n",
    ")\n",
    "calibrator.fit(props_val, val.y.numpy())\n",
    "\n",
    "# 3) use it to get calibrated probabilities on your test set\n",
    "probs_calibrated = calibrator.predict_proba(probs_test)  # shape (n_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables calibrator\n",
    "C = 0.03\n",
    "solver = \"lbfgs\"\n",
    "\n",
    "# get raw logits from the MLP\n",
    "props_val = model.predict_proba(val.X)  # shape (n_val, n_classes)\n",
    "probs_test = model.predict_proba(test.X)  # shape (n_test, n_classes)\n",
    "\n",
    "# instantiate & fit the Platt calibrator\n",
    "platt_calibrator = PlattCalibrator(C=C, solver=solver, multi_class=\"multinomial\", log_transform=True)\n",
    "platt_calibrator.fit(props_val, val.y.numpy())\n",
    "\n",
    "# use it to get calibrated probabilities on your test set\n",
    "probs_calibrated_platt = platt_calibrator.predict_proba(probs_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Compute calibration metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 5 # ECE is based on bins\n",
    "\n",
    "# compute ECEs\n",
    "for metric in conf_ECE, classwise_ECE:  # ECE,\n",
    "    print(metric.__name__)\n",
    "    print(\"Classifier = {:.3f}\".format(metric(test.y.numpy(), probs_test, bins=bins)))\n",
    "    print(\n",
    "        \"Calibrator = {:.3f}\".format(metric(test.y.numpy(), probs_calibrated, bins=bins))\n",
    "    )\n",
    "    print(\"\")\n",
    "\n",
    "# compute brier score loss\n",
    "true_corr = (np.argmax(probs_test, axis=1) == y_test).astype(int)\n",
    "print('brier_score_loss')\n",
    "print(\"Classifier       = {:.3f}\".format(brier_score_loss(true_corr, probs_test.max(axis=1))))\n",
    "print(\"Calibrated      = {:.3f}\".format(brier_score_loss(true_corr, probs_calibrated.max(axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Plot reliability diagram (Baseline vs. Platt's scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reliability diagram\n",
    "fig = plot_reliability_diagram(\n",
    "    labels=y_test,\n",
    "    scores=\n",
    "    [\n",
    "        probs_test,\n",
    "        probs_calibrated\n",
    "    ],\n",
    "    legend=[\n",
    "        \"MLP (reduced label space)\", \"Calibrated\"\n",
    "    ],\n",
    "    # show_gaps=True,\n",
    "    # show_histogram=True,\n",
    "    confidence=True,\n",
    "    bins=11,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Compute test scores (comparing baseline to platt's scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert calibrated probabilities to predicted classes\n",
    "y_pred_base = torch.argmax(torch.tensor(probs_test), dim=1)\n",
    "\n",
    "# Initialize metrics\n",
    "f1_macro = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "recall_macro = Recall(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "\n",
    "# Update metrics with predictions and true labels\n",
    "f1_macro.update(y_pred_base, test.y)\n",
    "recall_macro.update(y_pred_base, test.y)\n",
    "\n",
    "# Compute final values\n",
    "final_f1 = f1_macro.compute()\n",
    "final_recall = recall_macro.compute()\n",
    "\n",
    "# Print results\n",
    "print(\"Test scores of the basemodel\")\n",
    "print(f\"F1 Macro: {final_f1:.4f}\")\n",
    "print(f\"Recall Macro: {final_recall:.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "# Convert calibrated probabilities to predicted classes\n",
    "y_pred_calibrated = torch.argmax(torch.tensor(probs_calibrated), dim=1)\n",
    "\n",
    "# Initialize metrics\n",
    "f1_macro = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "recall_macro = Recall(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "\n",
    "# Update metrics with predictions and true labels\n",
    "f1_macro.update(y_pred_calibrated, test.y)\n",
    "recall_macro.update(y_pred_calibrated, test.y)\n",
    "\n",
    "# Compute final values\n",
    "final_f1 = f1_macro.compute()\n",
    "final_recall = recall_macro.compute()\n",
    "\n",
    "# Print results\n",
    "print(\"Test scores of the calibrated improvement\")\n",
    "print(f\"F1 Macro: {final_f1:.4f}\")\n",
    "print(f\"Recall Macro: {final_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Temprature scaling\n",
    "Multiclass platt's scaling is more flexible but potentially more data-hungry (one weight + bias per class). temperature scaling is based on a single‐parameter rescaling (Single scalar T that uniformly “softens” or “sharpens” all logits), therefore it might better in the current setting. The drawback is its low flexibility because of the single parameter it can only scale calibration globaly instead of each class seperately. Therefore also lower risk on overfitting. Therefore, it is promising when the network is systematically over- or under-confident across all classes, which is the case. Suitable when in case of a small validation set.\n",
    "Platt's scaling is more suited for class-specific miscallibration (not the case given it is under-confident accross all) and when having plenty of validation data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract logits on val & test\n",
    "logits_val = model.predict_logits(val.X)  # shape (n_val, n_classes)\n",
    "logits_test = model.predict_logits(test.X)  # shape (n_test, n_classes)\n",
    "\n",
    "# fit temperature\n",
    "temp_scaler = TemperatureScaling(device=next(model.parameters()).device)\n",
    "temp_scaler.fit(logits_val, val.y.numpy())\n",
    "\n",
    "# get calibrated probabilities\n",
    "probs_temp = temp_scaler.predict_proba(logits_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Compute calibration metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ECEs\n",
    "for metric in (conf_ECE, classwise_ECE):\n",
    "    print(metric.__name__)\n",
    "    print(\n",
    "        \"Classifier       = {:.3f}\".format(metric(test.y.numpy(), probs_test, bins=bins))\n",
    "    )\n",
    "    print(\n",
    "        \"Temp-scaled      = {:.3f}\".format(metric(test.y.numpy(), probs_temp, bins=bins))\n",
    "    )\n",
    "    print(\"\")\n",
    "\n",
    "# compute brier score loss\n",
    "true_corr = (np.argmax(probs_test, axis=1) == y_test).astype(int)\n",
    "print('brier_score_loss')\n",
    "print(\"Classifier       = {:.3f}\".format(brier_score_loss(true_corr, probs_test.max(axis=1))))\n",
    "print(\"Temp-scaled      = {:.3f}\".format(brier_score_loss(true_corr, probs_temp.max(axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Compute test scores (comparing baseline to temperature scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert calibrated probabilities to predicted classes\n",
    "y_pred_base = torch.argmax(torch.tensor(probs_test), dim=1)\n",
    "\n",
    "# Initialize metrics\n",
    "f1_macro = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "recall_macro = Recall(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "\n",
    "# Update metrics with predictions and true labels\n",
    "f1_macro.update(y_pred_base, test.y)\n",
    "recall_macro.update(y_pred_base, test.y)\n",
    "\n",
    "# Compute final values\n",
    "final_f1 = f1_macro.compute()\n",
    "final_recall = recall_macro.compute()\n",
    "\n",
    "# Print results\n",
    "print(\"Test scores of the basemodel\")\n",
    "print(f\"F1 Macro: {final_f1:.4f}\")\n",
    "print(f\"Recall Macro: {final_recall:.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "# Convert calibrated probabilities to predicted classes\n",
    "y_pred_calibrated = torch.argmax(torch.tensor(probs_temp), dim=1)\n",
    "\n",
    "# Initialize metrics\n",
    "f1_macro = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "recall_macro = Recall(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "\n",
    "# Update metrics with predictions and true labels\n",
    "f1_macro.update(y_pred_calibrated, test.y)\n",
    "recall_macro.update(y_pred_calibrated, test.y)\n",
    "\n",
    "# Compute final values\n",
    "final_f1 = f1_macro.compute()\n",
    "final_recall = recall_macro.compute()\n",
    "\n",
    "# Print results\n",
    "print(\"Test scores of the calibrated improvement\")\n",
    "print(f\"F1 Macro: {final_f1:.4f}\")\n",
    "print(f\"Recall Macro: {final_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Plot reliability diagram (baseline vs. Platt's vs. Temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reliability diagram\t\n",
    "fig = plot_reliability_diagram(\n",
    "    labels=y_test,\n",
    "    scores=\n",
    "    [\n",
    "        probs_test,\n",
    "        probs_calibrated,\n",
    "        probs_temp\n",
    "    ],\n",
    "    legend=[\n",
    "        \"MLP (original)\", \"Platt\", \"Temp\"\n",
    "    ],\n",
    "    # show_gaps=True,\n",
    "    # show_histogram=True,\n",
    "    confidence=True,\n",
    "    bins=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare ECEs\n",
    "print(\"Temp‐scaled ECE:\", conf_ECE(test.y.numpy(), probs_temp, bins=15))\n",
    "print(\"Logistic ECE:\", conf_ECE(test.y.numpy(), probs_calibrated, bins=15))\n",
    "\n",
    "# compare brier score loss\n",
    "print(\"Temp‐scaled brier score loss:\", brier_score_loss(true_corr, probs_temp.max(axis=1)))\n",
    "print(\"Logistic brier score loss:\", brier_score_loss(true_corr, probs_calibrated.max(axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
