{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### preprocessing\n",
    "- drop NaN values\n",
    "- removed 3rd gender (n=1)\n",
    "- merge rare classes\n",
    "- drop classes with less than 25 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative imports\n",
    "from uncertainty_aware_diagnosis import(\n",
    "    ICD10data, \n",
    "    SimpleMLP, \n",
    "    TemperatureScaling,\n",
    "    BaseCalibrator,\n",
    "    PlattCalibrator,\n",
    "    MulticlassTemperatureScaling,\n",
    "    TopLabelTemperatureScaling,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# absolute imports\n",
    "import polars as pl\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchmetrics import F1Score, Recall\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from pycalib.visualisations import plot_reliability_diagram\n",
    "from pycalib.models.calibrators import LogisticCalibration\n",
    "from pycalib.metrics import classwise_ECE, conf_ECE\n",
    "\n",
    "# paths\n",
    "train_csv = \"./data/lbz-train.csv\"\n",
    "val_csv = \"./data/lbz-val.csv\"\n",
    "test_csv = \"./data/lbz-test.csv\"\n",
    "ohe_pkl = \"./data/ohe_cats.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_hoofdgroepen_only = False\n",
    "use_subset = False\n",
    "use_single_hospital = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "if predict_hoofdgroepen_only:\n",
    "    target = \"hoofdgroep\"  # simplify multi-class problem by predicting hoofdgroepen instead of specific codes\n",
    "else:\n",
    "    target = \"icd10_main_code\"\n",
    "\n",
    "categorical = [\n",
    "    \"zorginstellingnaam\",\n",
    "    \"gender\",\n",
    "    \"clinical_specialty\",\n",
    "    \"DBC_specialty_code\",\n",
    "    \"DBC_diagnosis_code\",\n",
    "    \"icd10_subtraject_code\",\n",
    "]\n",
    "numerical = [\"age\"]\n",
    "\n",
    "# get one-hot encoded features of full dataset (all categories)\n",
    "# ohe_df = pl.read_csv(ohe_csv).to_pandas()['ohe_cats']\n",
    "# ohe_cats =[]\n",
    "# for cat in ohe_df:\n",
    "#     ohe_cats.append(cat)\n",
    "\n",
    "with open(ohe_pkl, \"rb\") as f:\n",
    "    ohe_cats = pickle.load(f)\n",
    "\n",
    "train = ICD10data(\n",
    "    csv_path=train_csv,\n",
    "    numerical=numerical,\n",
    "    categorical=categorical,\n",
    "    high_card=[],\n",
    "    target=target,\n",
    "    dropna=True,\n",
    "    use_embedding=False,\n",
    "    ohe_categories=ohe_cats,  # use one-hot encoded categorie of full dataset\n",
    ")\n",
    "val = ICD10data(\n",
    "    csv_path=val_csv,\n",
    "    numerical=numerical,\n",
    "    categorical=categorical,\n",
    "    high_card=[],\n",
    "    target=target,\n",
    "    dropna=True,\n",
    "    use_embedding=False,\n",
    "    ohe_categories=ohe_cats,  # use one-hot encoded categorie of full dataset\n",
    "    encoder=train.encoder,  # use encoder from training set\n",
    "    scaler=train.scaler,  # use scalor from train set\n",
    ")\n",
    "test = ICD10data(\n",
    "    csv_path=test_csv,\n",
    "    numerical=numerical,\n",
    "    categorical=categorical,\n",
    "    high_card=[],\n",
    "    target=target,\n",
    "    dropna=True,\n",
    "    use_embedding=False,\n",
    "    ohe_categories=ohe_cats,  # use one-hot encoded categorie of full dataset\n",
    "    encoder=train.encoder,  # use encoder from training set\n",
    "    scaler=train.scaler,  # use scalor from train set\n",
    ")\n",
    "\n",
    "input_dim = train.X.shape[1]\n",
    "output_dim = train.classes.shape[0]\n",
    "\n",
    "print(f\"Number of icd10 classes: {len(train.classes)}\")\n",
    "print(f\"(input_dim: {input_dim}, output_dim: {output_dim})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables dataloader\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "\n",
    "if use_subset:\n",
    "    train.X = train.X[:6400]\n",
    "    val.X = val.X[:6400]\n",
    "    train.y = train.y[:6400]\n",
    "    val.y = val.y[:6400]\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=shuffle)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### select single hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_single_hospital:\n",
    "    # load data\n",
    "    target = \"icd10_main_code\"\n",
    "    categorical = [\n",
    "        \"zorginstellingnaam\",\n",
    "        \"gender\",\n",
    "        \"clinical_specialty\",\n",
    "        \"DBC_specialty_code\",\n",
    "        \"DBC_diagnosis_code\",\n",
    "        \"icd10_subtraject_code\",\n",
    "    ]\n",
    "    numerical = [\"age\"]\n",
    "\n",
    "    # get one-hot encoded features of full dataset (all categories)\n",
    "    # ohe_df = pl.read_csv(ohe_csv).to_pandas()['ohe_cats']\n",
    "    # ohe_cats =[]\n",
    "    # for cat in ohe_df:\n",
    "    #     ohe_cats.append(cat)\n",
    "\n",
    "    with open(ohe_pkl, \"rb\") as f:\n",
    "        ohe_cats = pickle.load(f)\n",
    "\n",
    "    train = ICD10data(\n",
    "        csv_path=train_csv[:-4] + \"_1hosp\" + \".csv\",\n",
    "        numerical=numerical,\n",
    "        categorical=categorical,\n",
    "        high_card=[],\n",
    "        target=target,\n",
    "        dropna=True,\n",
    "        use_embedding=False,\n",
    "        ohe_categories=ohe_cats,  # use one-hot encoded categorie of full dataset\n",
    "    )\n",
    "    val = ICD10data(\n",
    "        csv_path=val_csv[:-4] + \"_1hosp\" + \".csv\",\n",
    "        numerical=numerical,\n",
    "        categorical=categorical,\n",
    "        high_card=[],\n",
    "        target=target,\n",
    "        dropna=True,\n",
    "        use_embedding=False,\n",
    "        ohe_categories=ohe_cats,  # use one-hot encoded categorie of full dataset\n",
    "        encoder=train.encoder,  # use encoder from training set\n",
    "        scaler=train.scaler,  # use scalor from train set\n",
    "    )\n",
    "    test = ICD10data(\n",
    "        csv_path=test_csv[:-4] + \"_1hosp\" + \".csv\",\n",
    "        numerical=numerical,\n",
    "        categorical=categorical,\n",
    "        high_card=[],\n",
    "        target=target,\n",
    "        dropna=True,\n",
    "        use_embedding=False,\n",
    "        ohe_categories=ohe_cats,  # use one-hot encoded categorie of full dataset\n",
    "        encoder=train.encoder,  # use encoder from training set\n",
    "        scaler=train.scaler,  # use scalor from train set\n",
    "    )\n",
    "\n",
    "    input_dim = train.X.shape[1]\n",
    "    output_dim = train.classes.shape[0]\n",
    "\n",
    "    print(f\"Number of icd10 classes: {len(train.classes)}\")\n",
    "    print(f\"(input_dim: {input_dim}, output_dim: {output_dim})\")\n",
    "\n",
    "    # variables dataloader\n",
    "    batch_size = 32\n",
    "    shuffle = True\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables MLP\n",
    "num_epochs = 150\n",
    "early_stopping_patience = 20\n",
    "learning_rate = 1e-3\n",
    "dropout = 0.2\n",
    "hidden_dim = 256\n",
    "\n",
    "model = SimpleMLP(\n",
    "    input_dim=input_dim, hidden_dim=hidden_dim, num_classes=output_dim, dropout=dropout\n",
    ")\n",
    "model.fit(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    early_stopping_patience=early_stopping_patience,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(test.X)\n",
    "y_proba = model.predict_proba(test.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test.y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = test.y.numpy()\n",
    "\n",
    "# # select y labels: the most common, the most rare, and middle\n",
    "# y_select_table = (\n",
    "#     pl.DataFrame(train.y.numpy()).to_series().value_counts(sort=True)[0, 500, -1]\n",
    "# )\n",
    "# y_select = list(y_select_table[\"column_0\"])\n",
    "# y_mask = np.isin(y_test, y_select)\n",
    "# y_test_select = y_test[y_mask]\n",
    "# y_proba_rows_select = y_proba[y_mask]\n",
    "# y_proba_select = y_proba_rows_select[:, y_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = plot_reliability_diagram(\n",
    "#     y_test_select,\n",
    "#     [\n",
    "#         y_proba_select,\n",
    "#     ],\n",
    "#     legend=[\n",
    "#         \"MLP\",\n",
    "#     ],\n",
    "#     class_names=list(test.classes[y_select]),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables calibrator\n",
    "C = 0.03\n",
    "solver = \"lbfgs\"\n",
    "\n",
    "# 1) get your raw logits from the MLP\n",
    "props_val = model.predict_proba(val.X)  # shape (n_val, n_classes)\n",
    "probs_test = model.predict_proba(test.X)  # shape (n_test, n_classes)\n",
    "\n",
    "# 2) instantiate & fit the pycalib logistic (Platt) calibrator\n",
    "calibrator = LogisticCalibration(\n",
    "    C=C, solver=solver, multi_class=\"multinomial\", log_transform=True\n",
    ")\n",
    "calibrator.fit(props_val, val.y.numpy())\n",
    "\n",
    "# 3) use it to get calibrated probabilities on your test set\n",
    "probs_calibrated = calibrator.predict_proba(probs_test)  # shape (n_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 5\n",
    "for metric in conf_ECE, classwise_ECE:  # ECE,\n",
    "    print(metric.__name__)\n",
    "    print(\"Classifier = {:.3f}\".format(metric(test.y.numpy(), probs_test, bins=bins)))\n",
    "    print(\n",
    "        \"Calibrator = {:.3f}\".format(metric(test.y.numpy(), probs_calibrated, bins=bins))\n",
    "    )\n",
    "    print(\"\")\n",
    "\n",
    "true_corr = (np.argmax(probs_test, axis=1) == y_test).astype(int)\n",
    "print('brier_score_loss')\n",
    "print(\"Classifier       = {:.3f}\".format(brier_score_loss(true_corr, probs_test.max(axis=1))))\n",
    "print(\"Calibrated      = {:.3f}\".format(brier_score_loss(true_corr, probs_calibrated.max(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_reliability_diagram(\n",
    "    labels=y_test,\n",
    "    scores=\n",
    "    [\n",
    "        probs_test,\n",
    "        probs_calibrated\n",
    "    ],\n",
    "    legend=[\n",
    "        \"MLP (reduced label space)\", \"Calibrated\"\n",
    "    ],\n",
    "    # show_gaps=True,\n",
    "    # show_histogram=True,\n",
    "    confidence=True,\n",
    "    bins=11,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert calibrated probabilities to predicted classes\n",
    "y_pred_base = torch.argmax(torch.tensor(probs_test), dim=1)\n",
    "\n",
    "# Initialize metrics\n",
    "f1_macro = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "recall_macro = Recall(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "\n",
    "# Update metrics with predictions and true labels\n",
    "f1_macro.update(y_pred_base, test.y)\n",
    "recall_macro.update(y_pred_base, test.y)\n",
    "\n",
    "# Compute final values\n",
    "final_f1 = f1_macro.compute()\n",
    "final_recall = recall_macro.compute()\n",
    "\n",
    "# Print results\n",
    "print(\"Test scores of the basemodel\")\n",
    "print(f\"F1 Macro: {final_f1:.4f}\")\n",
    "print(f\"Recall Macro: {final_recall:.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "# Convert calibrated probabilities to predicted classes\n",
    "y_pred_calibrated = torch.argmax(torch.tensor(probs_calibrated), dim=1)\n",
    "\n",
    "# Initialize metrics\n",
    "f1_macro = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "recall_macro = Recall(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "\n",
    "# Update metrics with predictions and true labels\n",
    "f1_macro.update(y_pred_calibrated, test.y)\n",
    "recall_macro.update(y_pred_calibrated, test.y)\n",
    "\n",
    "# Compute final values\n",
    "final_f1 = f1_macro.compute()\n",
    "final_recall = recall_macro.compute()\n",
    "\n",
    "# Print results\n",
    "print(\"Test scores of the calibrated improvement\")\n",
    "print(f\"F1 Macro: {final_f1:.4f}\")\n",
    "print(f\"Recall Macro: {final_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Temprature scaling\n",
    "Multiclass platt's scaling is more flexible but potentially more data-hungry (one weight + bias per class). temperature scaling is based on a single‐parameter rescaling (Single scalar T that uniformly “softens” or “sharpens” all logits), therefore it might better in the current setting. The drawback is its low flexibility because of the single parameter it can only scale calibration globaly instead of each class seperately. Therefore also lower risk on overfitting. Therefore, it is promising when the network is systematically over- or under-confident across all classes, which is the case. Suitable when in case of a small validation set.\n",
    "Platt's scaling is more suited for class-specific miscallibration (not the case given it is under-confident accross all) and when having plenty of validation data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract logits on val & test\n",
    "logits_val = model.predict_logits(val.X)  # shape (n_val, n_classes)\n",
    "logits_test = model.predict_logits(test.X)  # shape (n_test, n_classes)\n",
    "\n",
    "# fit temperature\n",
    "temp_scaler = TemperatureScaling(device=next(model.parameters()).device)\n",
    "temp_scaler.fit(logits_val, val.y.numpy())\n",
    "\n",
    "# get calibrated probabilities\n",
    "probs_temp = temp_scaler.predict_proba(logits_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compare on your selected subset exactly as you did before\n",
    "# y_proba_rows_select_test  = probs_test[y_mask]\n",
    "# y_proba_select_test      = y_proba_rows_select_test[:, y_select]\n",
    "# y_proba_rows_select_cali = probs_calibrated[y_mask]\n",
    "# y_proba_select_cali      = y_proba_rows_select_cali[:, y_select]\n",
    "\n",
    "# # re-plot reliability\n",
    "# _ = plot_reliability_diagram(\n",
    "#     y_test_select,\n",
    "#     [y_proba_select_test, y_proba_select_cali],\n",
    "#     legend=[\"MLP\", \"+ Temp-scale\"],\n",
    "#     class_names=list(test.classes[y_select]),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ECEs\n",
    "for metric in (conf_ECE, classwise_ECE):\n",
    "    print(metric.__name__)\n",
    "    print(\n",
    "        \"Classifier       = {:.3f}\".format(metric(test.y.numpy(), probs_test, bins=bins))\n",
    "    )\n",
    "    print(\n",
    "        \"Temp-scaled      = {:.3f}\".format(metric(test.y.numpy(), probs_temp, bins=bins))\n",
    "    )\n",
    "    print(\"\")\n",
    "\n",
    "true_corr = (np.argmax(probs_test, axis=1) == y_test).astype(int)\n",
    "print('brier_score_loss')\n",
    "print(\"Classifier       = {:.3f}\".format(brier_score_loss(true_corr, probs_test.max(axis=1))))\n",
    "print(\"Temp-scaled      = {:.3f}\".format(brier_score_loss(true_corr, probs_temp.max(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert calibrated probabilities to predicted classes\n",
    "y_pred_base = torch.argmax(torch.tensor(probs_test), dim=1)\n",
    "\n",
    "# Initialize metrics\n",
    "f1_macro = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "recall_macro = Recall(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "\n",
    "# Update metrics with predictions and true labels\n",
    "f1_macro.update(y_pred_base, test.y)\n",
    "recall_macro.update(y_pred_base, test.y)\n",
    "\n",
    "# Compute final values\n",
    "final_f1 = f1_macro.compute()\n",
    "final_recall = recall_macro.compute()\n",
    "\n",
    "# Print results\n",
    "print(\"Test scores of the basemodel\")\n",
    "print(f\"F1 Macro: {final_f1:.4f}\")\n",
    "print(f\"Recall Macro: {final_recall:.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "# Convert calibrated probabilities to predicted classes\n",
    "y_pred_calibrated = torch.argmax(torch.tensor(probs_temp), dim=1)\n",
    "\n",
    "# Initialize metrics\n",
    "f1_macro = F1Score(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "recall_macro = Recall(task=\"multiclass\", average=\"macro\", num_classes=len(test.classes))\n",
    "\n",
    "# Update metrics with predictions and true labels\n",
    "f1_macro.update(y_pred_calibrated, test.y)\n",
    "recall_macro.update(y_pred_calibrated, test.y)\n",
    "\n",
    "# Compute final values\n",
    "final_f1 = f1_macro.compute()\n",
    "final_recall = recall_macro.compute()\n",
    "\n",
    "# Print results\n",
    "print(\"Test scores of the calibrated improvement\")\n",
    "print(f\"F1 Macro: {final_f1:.4f}\")\n",
    "print(f\"Recall Macro: {final_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_reliability_diagram(\n",
    "    labels=y_test,\n",
    "    scores=\n",
    "    [\n",
    "        probs_test,\n",
    "        probs_calibrated,\n",
    "        probs_temp\n",
    "    ],\n",
    "    legend=[\n",
    "        \"MLP (original)\", \"Platt\", \"Temp\"\n",
    "    ],\n",
    "    # show_gaps=True,\n",
    "    # show_histogram=True,\n",
    "    confidence=True,\n",
    "    bins=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after fitting temp_scaler as above\n",
    "print(\"Temp‐scaled ECE:\", conf_ECE(test.y.numpy(), probs_temp, bins=15))\n",
    "print(\"Logistic ECE:\", conf_ECE(test.y.numpy(), probs_calibrated, bins=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# base calibrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a) Get validation & test data\n",
    "logits_val = model.predict_logits(val.X)  # (n_val, K)\n",
    "probs_val = model.predict_proba(val.X)  # (n_val, K)\n",
    "y_val = val.y.numpy()\n",
    "\n",
    "logits_test = model.predict_logits(test.X)  # (n_test, K)\n",
    "probs_test = model.predict_proba(test.X)  # (n_test, K)\n",
    "y_test = test.y.numpy()\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "#  b) Instantiate both\n",
    "calibrators = {\n",
    "    \"Multiclass‐TS\": MulticlassTemperatureScaling(device=device, init_temp=1.0),\n",
    "    \"TopLabel‐TS\": TopLabelTemperatureScaling(device=device, init_temp=1.0),\n",
    "}\n",
    "\n",
    "#  c) Fit & apply\n",
    "calibrated = {}\n",
    "for name, cal in calibrators.items():\n",
    "    cal.fit(logits_val, probs_val, y_val)\n",
    "    calibrated[name] = cal.predict_proba(logits_test, probs_test)\n",
    "\n",
    "#  d) Evaluate\n",
    "print(calibrated)\n",
    "\n",
    "# For multiclass you might compute ECE or NLL; for the example below we\n",
    "# just show top-label Brier for both (binary):\n",
    "true_corr = (np.argmax(probs_test, axis=1) == y_test).astype(int)\n",
    "print(\"=== Brier top-label ===\")\n",
    "print(\"Uncalibrated:\", brier_score_loss(true_corr, probs_test.max(axis=1)))\n",
    "print(\"TopLabel-TS:\", brier_score_loss(true_corr, calibrated[\"TopLabel-TS\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Group common, uncommon and rare classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get raw labels & probs\n",
    "# ------------------------------------------------\n",
    "# Assuming your ICD10data objects expose .y as a torch.Tensor\n",
    "y_train = train.y.numpy()  # shape (n_train,)\n",
    "y_test = test.y.numpy()  # shape (n_test,)\n",
    "probs_test = model.predict_proba(test.X)  # shape (n_test, num_classes)\n",
    "num_classes = probs_test.shape[1]\n",
    "\n",
    "# 2) Compute class frequencies & sorted order\n",
    "# ------------------------------------------------\n",
    "class_counts = np.bincount(y_train, minlength=num_classes)\n",
    "sorted_idxs = np.argsort(class_counts)[::-1]  # descending by count\n",
    "cum_counts = class_counts[sorted_idxs].cumsum()\n",
    "total = class_counts.sum()\n",
    "\n",
    "# find where cumulative hits 33% and 66% of total examples\n",
    "cut1 = np.searchsorted(cum_counts, total * 0.33)\n",
    "cut2 = np.searchsorted(cum_counts, total * 0.66)\n",
    "\n",
    "high_freq_idxs = sorted_idxs[:cut1]\n",
    "mid_freq_idxs = sorted_idxs[cut1:cut2]\n",
    "low_freq_idxs = sorted_idxs[cut2:]\n",
    "\n",
    "# 3) Randomly sample up to K from each bucket\n",
    "# ------------------------------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "K_high, K_mid, K_low = 5, 5, 5\n",
    "\n",
    "sel_high = rng.choice(\n",
    "    high_freq_idxs, size=min(K_high, len(high_freq_idxs)), replace=False\n",
    ")\n",
    "sel_mid = rng.choice(mid_freq_idxs, size=min(K_mid, len(mid_freq_idxs)), replace=False)\n",
    "sel_low = rng.choice(low_freq_idxs, size=min(K_low, len(low_freq_idxs)), replace=False)\n",
    "\n",
    "selected_classes = np.concatenate([sel_high, sel_mid, sel_low])\n",
    "\n",
    "# 4) Compute per-class calibration curves\n",
    "# ------------------------------------------------\n",
    "n_bins = 15\n",
    "calib_data = {}\n",
    "\n",
    "for cls in selected_classes:\n",
    "    # binary ground-truth for “is this class?”\n",
    "    y_true_bin = (y_test == cls).astype(int)\n",
    "    # predicted probability for that class\n",
    "    y_prob_cls = probs_test[:, cls]\n",
    "\n",
    "    prob_pred, frac_true = calibration_curve(\n",
    "        y_true_bin, y_prob_cls, n_bins=n_bins, strategy=\"uniform\"\n",
    "    )\n",
    "    calib_data[cls] = (prob_pred, frac_true)\n",
    "\n",
    "# 5) Plot them together\n",
    "# ------------------------------------------------\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Ideal\")\n",
    "\n",
    "for cls in selected_classes:\n",
    "    prob_pred, frac_true = calib_data[cls]\n",
    "    plt.plot(\n",
    "        prob_pred,\n",
    "        frac_true,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        label=f\"class {cls} (freq={class_counts[cls]})\",\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.title(f\"Reliability diagram for {len(selected_classes)} sample classes\")\n",
    "plt.legend(loc=\"lower right\", fontsize=\"small\", ncol=1)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
